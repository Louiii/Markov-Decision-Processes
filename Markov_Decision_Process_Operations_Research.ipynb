{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Markov Decision Process - Operations Research.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Louiii/Markov-Decision-Processes/blob/master/Markov_Decision_Process_Operations_Research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ElrMDYtvGG1T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Markov Decision Processes"
      ]
    },
    {
      "metadata": {
        "id": "jl4DQ3PhGd-r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Policy Improvement Algorithm"
      ]
    },
    {
      "metadata": {
        "id": "QRpxDsXQGbaM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "These dynamic programming examples do not have a definite final stage from which we work back via backwards induction. These are problems with no natural final stage: infinite horizon problems. These problems are only manageable if we impose some additional structure on how the stages progress; some version of the Markov property is what we need here. Markov chains can be used to model the evolution of systems that change in some random fashion which may depend on their current state but not on their earlier history. These are Markov decision processes, a useful and important class of models which allow us to take actions which at some known cost modify the transition probabilities of a system. The question of interest is how to optimally choose which actions to take."
      ]
    },
    {
      "metadata": {
        "id": "2cSDDXEKGHVq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MDP:\n",
        "    def __init__(self, markov_chains, rewards, actions, states):\n",
        "        self.m = len(states)\n",
        "        self.states = states\n",
        "        self.actions = actions\n",
        "        expected_reward = []\n",
        "        for action in actions:\n",
        "            e = []\n",
        "            for i in range(self.m):\n",
        "                e.append(sum( [rewards[action][i][j] * markov_chains[action][i][j] for j in range(self.m)]) )\n",
        "            expected_reward.append(e)\n",
        "        self.expected_reward = np.array(expected_reward)\n",
        "        self.markov_chains = markov_chains\n",
        "        self.iteration_count = 0\n",
        "        \n",
        "\n",
        "\n",
        "    def policy_iteration(self, A):\n",
        "        self.iteration_count += 1\n",
        "        r, V = [], []\n",
        "        for state in states:# for each state i...\n",
        "            r.append( -self.expected_reward[A[state]][state] ) # rewards[A[i]][i, A[i]] \n",
        "            V.append( [markov_chains[A[state]][state, j] for j in states[:-1]] )\n",
        "        V = np.array(V)  \n",
        "        r = np.array(r)    \n",
        "        X = np.zeros((self.m, self.m))\n",
        "        for i in range(self.m): \n",
        "            X[i, -1] = -1\n",
        "            X[i,  i] = -1\n",
        "            for j in states[:-1]:\n",
        "                X[i, j] += V[i, j]\n",
        "        V = np.linalg.solve(X, r)\n",
        "        lambda_ = V[-1]\n",
        "        T = []\n",
        "        for state in states:\n",
        "            t=[]\n",
        "            for action in actions:\n",
        "                VP = sum( [markov_chains[action][state, j] * V[j] for j in states[:-1]] )\n",
        "                t.append( self.expected_reward[action][state] + VP ) # rewards[A[state]][state, action]\n",
        "            T.append(t)\n",
        "        improved_policy = []\n",
        "        for i in states:\n",
        "            improved_policy.append( T[i].index(max(T[i])) )\n",
        "        return improved_policy, lambda_\n",
        "    \n",
        "    def iterate(self, A):\n",
        "        A1, lmbd = self.policy_iteration(A)\n",
        "        while (A != A1):\n",
        "            print('\\n\\nNew policy: '+str(A1)+' (iteration '+str(self.iteration_count)+')\\n\\n')\n",
        "            A = A1\n",
        "            A1, lmbd = self.policy_iteration(A)\n",
        "        return A, lmbd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bT66sEFhGKQ1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Op Research Q.83"
      ]
    },
    {
      "metadata": {
        "id": "v5GeAzMCB1hq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "83 (*). \n",
        "A tennis player, on serve, has two attempts at serving to win the point, and on each of the first and second serve can either serve hard or safe. A hard serve has probability 1/4 of going in, and if it does go in the player has probability 2/3 of winning the point. A safe serve has probability 4/5 of going in, and if it does go in the player has probability 1/2 of winning the point.\n",
        "The server wants to optimize their service strategy over a long sequence of serves using the following simple reward system: the player gains 1 by winning the point and −1 by losing it (including by serving a double fault, i.e., if a second serve does not go in). If the outcome of the point is yet to be decided (e.g. if a first serve goes out) there is no immediate reward.\n",
        "Formulate this problem as a Markov decision process with two states (first or second serve) and two actions (hard or safe). Write down the transition matrices associated with each of the two actions, and calculate the expected immediate rewards r(i,k) on performing action k in state i.\n",
        "For the expected rewards, you should use the partition theorem over whether the serve is in or out; for example, you\n",
        "should find that $r (0, 0) = \\frac{1}{4}(\\frac{1}{3}\\cdot (-1) + \\frac{2}{3}\\cdot (+1)) + \\frac{3}{4}\\cdot 0 = \\frac{1}{12}$\n",
        "The aim is to maximize the long-run average reward per service.\n",
        "Find the optimal strategy by exhaustion over the four possible policies."
      ]
    },
    {
      "metadata": {
        "id": "9hyTtPFMAiW0",
        "colab_type": "code",
        "outputId": "3a44ea84-214b-4659-fc5e-cca4b851af9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "actions = {0, 1}# hard, soft\n",
        "states  = [0, 1,2]# 1st serve, 2nd serve, In\n",
        "P1 = np.array([[0, 3/4, 1/4],\n",
        "               [3/4, 0, 1/4],\n",
        "               [1,0,0]])\n",
        "P2 = np.array([[0, 1/5, 4/5],\n",
        "               [1/5,0, 4/5],\n",
        "               [1,0,0]])\n",
        "R1 = np.array([[0, 0, 0],\n",
        "               [-1, 0, 0],\n",
        "               [1/3, 0, 0]])\n",
        "R2 = np.array([[0, 0, 0],\n",
        "               [-1, 0, 0],\n",
        "               [0, 0, 0]])\n",
        "rewards = { 0: R1, 1: R2}\n",
        "markov_chains = { 0: P1, 1: P2}\n",
        "initial_policy = [0, 0, 0]# In state 0, do action 0. In state 1, do action 0.  In state 2, do action 0. \n",
        "\n",
        "mdp_Q83 = MDP(markov_chains, rewards, actions, states)\n",
        "final_policy_Q83, lmbd = mdp_Q83.iterate(initial_policy)\n",
        "print(final_policy_Q83)\n",
        "print(\"Long run expected reward: \"+str(lmbd))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "New policy: [1, 1, 0] (iteration 1)\n",
            "\n",
            "\n",
            "[1, 1, 0]\n",
            "Long run expected reward: 0.12962962962962962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "khkAtohHA8cG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Operations Research Example 9.1"
      ]
    },
    {
      "metadata": {
        "id": "BjYXK2wnG--B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The manager of a market garden knows that output over the growing season is affected greatly by soil conditions in spring. The income of the business depends on output but also running costs e.g. it is possible at some cost to improve the soil condition by testing it and using fertilizers to restore missing nutrients or improving drainage etc. To reflect the fact that it takes quite a while for changes of state to occur the manager will estimate revenue for each pair (current state, next state). Let’s keep the problem simple: there are only three soil conditions good, fair and bad which we’ll denote 0,1, 2; the possible actions on the soil are to leave or improve it which we’ll label 0,1. The transition probabilities from season to season are\n",
        "\n",
        "\n",
        "\n",
        "P(0) = [P(i, j;0)] = \\begin{bmatrix}\n",
        "    0.2 & 0.5 & 0.3 \\\\0 & 0.5 & 0.5 \\\\0 & 0 & 1 \n",
        "  \\end{bmatrix}\n",
        "  \n",
        " P(1) = [P(i, j;1)] = \\begin{bmatrix}\n",
        "    0.3 & 0.6 & 0.1 \\\\0.1 & 0.6 & 0.3 \\\\0.05 & 0.4 & 0.55 \n",
        "  \\end{bmatrix}\n",
        "\n",
        "The one-year revenues are\n",
        "\n",
        "R(0) = [R(i, j;0)] = \\begin{bmatrix}\n",
        "    7 & 6 & 5 \\\\0 & 5 & 1 \\\\0 & 0 & -1 \n",
        "  \\end{bmatrix}\n",
        "  \n",
        " R(1) = [R(i, j;1)] = \\begin{bmatrix}\n",
        "    6 & 5 & -1 \\\\7 & 4 & 0 \\\\6 & 3 & -2 \n",
        "  \\end{bmatrix}\n",
        "\n",
        "\n",
        "The manager would like to know when it’s best to improve the soil condition."
      ]
    },
    {
      "metadata": {
        "id": "3-_-KhO9AfhD",
        "colab_type": "code",
        "outputId": "f9d35c2a-e584-4e4a-c095-6db65d862794",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "actions = {0, 1}#label actions starting from 0\n",
        "states  = [0, 1, 2]#label states starting from 0\n",
        "P1 = np.array([[0.2, 0.5, 0.3],\n",
        "               [  0, 0.5, 0.5],\n",
        "               [  0,   0,   1]])\n",
        "P2 = np.array([[0.3, 0.6, 0.1],\n",
        "               [0.1, 0.6, 0.3],\n",
        "               [0.05,0.4,0.55]])\n",
        "R1 = np.array([[7, 6, 3],\n",
        "               [0, 5, 1],\n",
        "               [0, 0,-1]])\n",
        "R2 = np.array([[6, 5,-1],\n",
        "               [7, 4, 0],\n",
        "               [6, 3,-2]])\n",
        "rewards = { 0: R1, 1: R2}\n",
        "markov_chains = { 0: P1, 1: P2 }\n",
        "initial_policy = [0, 0, 1]# In state 1, do action 1. In state 2, do action 1. \n",
        "\n",
        "\n",
        "mdp_Ex9_1 = MDP(markov_chains, rewards, actions, states)\n",
        "final_policy_Ex9_1, lmbd = mdp_Ex9_1.iterate(initial_policy)\n",
        "print( final_policy_Ex9_1 )\n",
        "print(\"Long run expected reward: \"+str(lmbd))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "New policy: [1, 1, 1] (iteration 1)\n",
            "\n",
            "\n",
            "[1, 1, 1]\n",
            "Long run expected reward: 2.255932203389831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rO6-u5fAbeA4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Question 85:\n"
      ]
    },
    {
      "metadata": {
        "id": "YXw6xko4bUSo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the quiz show The Missing Link questions are of type 1, 2, 3 or 4. The value of a type i question is 100 for i = 1, 200 for i = 2, 500 for i = 3, 1000 for i = 4. Your probability of correctly answering a question of type i is 1−(0.1×i).\n",
        "After a correctly answered type i question (i < 4), you can either bank the value of that question (in which case the next question is of type 1), or wait for the next question (in which case you bank nothing but the next question is of type i + 1). If you correctly answer a type 4 question you automatically bank 1000 and return to type 1. If you answer incorrectly (or pass) you bank nothing and return to type 1.\n",
        "You wish to maximise the long-run amount banked per question asked.\n",
        "(i) Formulate this problem as a Markov decision process. Hint: Use states {1, 2, 3, 4} for the type of question about\n",
        "to be asked, and four non-zero rewards (where reward = money in the bank).\n",
        "(ii) Solve the problem using the policy improvement algorithm starting with the policy of banking before a question of type 4."
      ]
    },
    {
      "metadata": {
        "id": "jemATK5dbyHY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "e2ee0bbb-73c6-42fb-a967-29961c13ac72"
      },
      "cell_type": "code",
      "source": [
        "actions = {0, 1}#answer, bank\n",
        "states  = [0, 1, 2, 3]#each question currently on\n",
        "P1 = np.array([[0.1, 0.9, 0, 0],\n",
        "               [0.2, 0, 0.8, 0],\n",
        "               [0.3, 0, 0, 0.7],\n",
        "               [1, 0, 0, 0]])\n",
        "P2 = np.array([[1, 0, 0, 0],\n",
        "               [1, 0, 0, 0],\n",
        "               [1, 0, 0, 0],\n",
        "               [1, 0, 0, 0]])\n",
        "R1 = np.array([[0, 0, 0, 0],\n",
        "               [0, 0, 0, 0],\n",
        "               [0, 0, 0, 0],\n",
        "               [600, 0, 0, 0]])\n",
        "R2 = np.array([[0, 0, 0, 0],\n",
        "               [100, 0, 0, 0],\n",
        "               [200, 0, 0, 0],\n",
        "               [500, 0, 0, 0]])\n",
        "rewards = { 0: R1, 1: R2}\n",
        "markov_chains = { 0: P1, 1: P2 }\n",
        "initial_policy = [0, 0, 0, 1]\n",
        "\n",
        "\n",
        "\n",
        "mdp_q85 = MDP(markov_chains, rewards, actions, states)\n",
        "final_policy_q85, lmbd = mdp_q85.iterate(initial_policy)\n",
        "print( final_policy_q85 )\n",
        "print(\"Long run expected reward: \"+str(lmbd))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "New policy: [0, 0, 0, 0] (iteration 1)\n",
            "\n",
            "\n",
            "[0, 0, 0, 0]\n",
            "Long run expected reward: 96.7989756722151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "trtR4Qshc-_A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}