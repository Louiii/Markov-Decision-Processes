{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Markov Decision Process - Operations Research.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Louiii/Markov-Decision-Processes/blob/master/Markov_Decision_Process_Operations_Research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ElrMDYtvGG1T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Markov Decision Processes"
      ]
    },
    {
      "metadata": {
        "id": "jl4DQ3PhGd-r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Policy Improvement Algorithm"
      ]
    },
    {
      "metadata": {
        "id": "QRpxDsXQGbaM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "These dynamic programming examples do not have a definite final stage from which we work back via backwards induction. These are problems with no natural final stage: infinite horizon problems. These problems are only manageable if we impose some additional structure on how the stages progress; some version of the Markov property is what we need here. Markov chains can be used to model the evolution of systems that change in some random fashion which may depend on their current state but not on their earlier history. These are Markov decision processes, a useful and important class of models which allow us to take actions which at some known cost modify the transition probabilities of a system. The question of interest is how to optimally choose which actions to take."
      ]
    },
    {
      "metadata": {
        "id": "2cSDDXEKGHVq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MDP:\n",
        "    def __init__(self, markov_chains, rewards, actions, states):\n",
        "        self.m = len(states)\n",
        "        self.states = states\n",
        "        self.actions = actions\n",
        "        expected_reward = []\n",
        "        for action in actions:\n",
        "            e = []\n",
        "            for i in range(self.m):\n",
        "                e.append(sum( [rewards[action][i][j] * markov_chains[action][i][j] for j in range(self.m)]) )\n",
        "            expected_reward.append(e)\n",
        "        self.expected_reward = np.array(expected_reward)\n",
        "        self.markov_chains = markov_chains\n",
        "        self.iteration_count = 0\n",
        "        \n",
        "\n",
        "\n",
        "    def policy_iteration(self, A):\n",
        "        self.iteration_count += 1\n",
        "        r, V = [], []\n",
        "        for state in states:# for each state i...\n",
        "            r.append( -self.expected_reward[A[state]][state] ) # rewards[A[i]][i, A[i]] \n",
        "            V.append( [markov_chains[A[state]][state, j] for j in states[:-1]] )\n",
        "        V = np.array(V)  \n",
        "        r = np.array(r)    \n",
        "        X = np.zeros((self.m, self.m))\n",
        "        for i in range(self.m): \n",
        "            X[i, -1] = -1\n",
        "            X[i,  i] = -1\n",
        "            for j in states[:-1]:\n",
        "                X[i, j] += V[i, j]\n",
        "        V = np.linalg.solve(X, r)\n",
        "        T = []\n",
        "        for state in states:\n",
        "            t=[]\n",
        "            for action in actions:\n",
        "                VP = sum( [markov_chains[action][state, j] * V[j] for j in states[:-1]] )\n",
        "                t.append( self.expected_reward[action][state] + VP ) # rewards[A[state]][state, action]\n",
        "            T.append(t)\n",
        "        improved_policy = []\n",
        "        for i in states:\n",
        "            improved_policy.append( T[i].index(max(T[i])) )\n",
        "        return improved_policy\n",
        "    \n",
        "    def iterate(self, A):\n",
        "        A1 = self.policy_iteration(A)\n",
        "        while (A != A1):\n",
        "            print('\\n\\nNew policy: '+str(A1)+' (iteration '+str(self.iteration_count)+')\\n\\n')\n",
        "            A = A1\n",
        "            A1 = self.policy_iteration(A)\n",
        "        return A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bT66sEFhGKQ1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Op Research Q.83"
      ]
    },
    {
      "metadata": {
        "id": "v5GeAzMCB1hq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "83 (*). \n",
        "A tennis player, on serve, has two attempts at serving to win the point, and on each of the first and second serve can either serve hard or safe. A hard serve has probability 1/4 of going in, and if it does go in the player has probability 2/3 of winning the point. A safe serve has probability 4/5 of going in, and if it does go in the player has probability 1/2 of winning the point.\n",
        "The server wants to optimize their service strategy over a long sequence of serves using the following simple reward system: the player gains 1 by winning the point and −1 by losing it (including by serving a double fault, i.e., if a second serve does not go in). If the outcome of the point is yet to be decided (e.g. if a first serve goes out) there is no immediate reward.\n",
        "Formulate this problem as a Markov decision process with two states (first or second serve) and two actions (hard or safe). Write down the transition matrices associated with each of the two actions, and calculate the expected immediate rewards r(i,k) on performing action k in state i.\n",
        "For the expected rewards, you should use the partition theorem over whether the serve is in or out; for example, you\n",
        "should find that $r (0, 0) = \\frac{1}{4}(\\frac{1}{3}\\cdot (-1) + \\frac{2}{3}\\cdot (+1)) + \\frac{3}{4}\\cdot 0 = \\frac{1}{12}$\n",
        "The aim is to maximize the long-run average reward per service.\n",
        "Find the optimal strategy by exhaustion over the four possible policies."
      ]
    },
    {
      "metadata": {
        "id": "9hyTtPFMAiW0",
        "colab_type": "code",
        "outputId": "928a6eef-2070-4607-e2dc-8dbfd449fcf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "actions = {0, 1}#label actions starting from 0\n",
        "states  = [0, 1]#label states starting from 0\n",
        "P1 = np.array([[0.25, 0.75],\n",
        "               [   1,    0]])\n",
        "P2 = np.array([[ 0.8,  0.2],\n",
        "               [   1,    0]])\n",
        "R  = np.array([[1/12,    0],\n",
        "               [-2/3, -1/5]])\n",
        "R1, R2 = np.array( [ list(R[:,0]), list(R[:,0]) ] ).T, np.array( [ list(R[:,1]), list(R[:,1]) ] ).T\n",
        "rewards = { 0: R1, 1: R2 }\n",
        "markov_chains = { 0: P1, 1: P2 }\n",
        "initial_policy = [0, 0]# In state 1, do action 1. In state 2, do action 1. \n",
        "\n",
        "mdp_Q83 = MDP(markov_chains, rewards, actions, states)\n",
        "final_policy_Q83 = mdp_Q83.iterate(initial_policy)\n",
        "print(final_policy_Q83)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "New policy: [1, 1] (iteration 1)\n",
            "\n",
            "\n",
            "[1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "khkAtohHA8cG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Operations Research Example 9.1"
      ]
    },
    {
      "metadata": {
        "id": "BjYXK2wnG--B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The manager of a market garden knows that output over the growing season is affected greatly by soil conditions in spring. The income of the business depends on output but also running costs e.g. it is possible at some cost to improve the soil condition by testing it and using fertilizers to restore missing nutrients or improving drainage etc. To reflect the fact that it takes quite a while for changes of state to occur the manager will estimate revenue for each pair (current state, next state). Let’s keep the problem simple: there are only three soil conditions good, fair and bad which we’ll denote 0,1, 2; the possible actions on the soil are to leave or improve it which we’ll label 0,1. The transition probabilities from season to season are\n",
        "\n",
        "\n",
        "\n",
        "P(0) = [P(i, j;0)] = \\begin{bmatrix}\n",
        "    0.2 & 0.5 & 0.3 \\\\0 & 0.5 & 0.5 \\\\0 & 0 & 1 \n",
        "  \\end{bmatrix}\n",
        "  \n",
        " P(1) = [P(i, j;1)] = \\begin{bmatrix}\n",
        "    0.3 & 0.6 & 0.1 \\\\0.1 & 0.6 & 0.3 \\\\0.05 & 0.4 & 0.55 \n",
        "  \\end{bmatrix}\n",
        "\n",
        "The one-year revenues are\n",
        "\n",
        "R(0) = [R(i, j;0)] = \\begin{bmatrix}\n",
        "    7 & 6 & 5 \\\\0 & 5 & 1 \\\\0 & 0 & -1 \n",
        "  \\end{bmatrix}\n",
        "  \n",
        " R(1) = [R(i, j;1)] = \\begin{bmatrix}\n",
        "    6 & 5 & -1 \\\\7 & 4 & 0 \\\\6 & 3 & -2 \n",
        "  \\end{bmatrix}\n",
        "\n",
        "\n",
        "The manager would like to know when it’s best to improve the soil condition."
      ]
    },
    {
      "metadata": {
        "id": "3-_-KhO9AfhD",
        "colab_type": "code",
        "outputId": "fe3a31dc-e849-4c33-daf1-8219337f8122",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "actions = {0, 1}#label actions starting from 0\n",
        "states  = [0, 1, 2]#label states starting from 0\n",
        "P1 = np.array([[0.2, 0.5, 0.3],\n",
        "               [  0, 0.5, 0.5],\n",
        "               [  0,   0,   1]])\n",
        "P2 = np.array([[0.3, 0.6, 0.1],\n",
        "               [0.1, 0.6, 0.3],\n",
        "               [0.05,0.4,0.55]])\n",
        "R1 = np.array([[7, 6, 3],\n",
        "               [0, 5, 1],\n",
        "               [0, 0,-1]])\n",
        "R2 = np.array([[6, 5,-1],\n",
        "               [7, 4, 0],\n",
        "               [6, 3,-2]])\n",
        "rewards = { 0: R1, 1: R2}\n",
        "markov_chains = { 0: P1, 1: P2 }\n",
        "initial_policy = [0, 0, 1]# In state 1, do action 1. In state 2, do action 1. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "mdp_Ex9_1 = MDP(markov_chains, rewards, actions, states)\n",
        "final_policy_Ex9_1 = mdp_Ex9_1.iterate(initial_policy)\n",
        "print( final_policy_Ex9_1 )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "New policy: [1, 1, 1] (iteration 1)\n",
            "\n",
            "\n",
            "[1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}